{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.keras\n",
    "\n",
    "from faculty import datasets\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Article type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_size):\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(split, max_words=1000):\n",
    "    data = pd.read_csv(\"/project/bbc-text.csv\")\n",
    "    \n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=max_words, \n",
    "        char_level=False\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    if split:\n",
    "        train_size = int(len(data) * .8)\n",
    "        print (\"Train size: %d\" % train_size)\n",
    "        print (\"Test size: %d\" % (len(data) - train_size))\n",
    "        # Split data\n",
    "        train_cat, test_cat = train_test_split(data['category'], train_size)\n",
    "        train_text, test_text = train_test_split(data['text'], train_size)\n",
    "    else:\n",
    "        print (\"Train size: %d\" % len(data))\n",
    "        train_cat = data['category']\n",
    "        train_text = data['text']\n",
    "        \n",
    "    # fit tokenizer to our training text data\n",
    "    tokenizer.fit_on_texts(train_text) \n",
    "    x_train = tokenizer.texts_to_matrix(train_text)\n",
    "    if split:\n",
    "        x_test = tokenizer.texts_to_matrix(test_text)\n",
    "\n",
    "    # Use sklearn utility to convert label strings to numbered index\n",
    "    encoder.fit(train_cat)\n",
    "    y_train = encoder.transform(train_cat)\n",
    "    if split:\n",
    "        y_test = encoder.transform(test_cat)\n",
    "\n",
    "    # Converts the labels to a one-hot representation\n",
    "    num_classes = np.max(y_train) + 1\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    if split:\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "    if split:\n",
    "        print('x_train shape:', x_train.shape)\n",
    "        print('x_test shape:', x_test.shape)\n",
    "        print('y_train shape:', y_train.shape)\n",
    "        print('y_test shape:', y_test.shape)\n",
    "        return x_train, x_test, y_train, y_test, tokenizer, encoder, num_classes\n",
    "    else:\n",
    "        print('x_train shape:', x_train.shape)\n",
    "        print('y_train shape:', y_train.shape)\n",
    "        return x_train, y_train, tokenizer, encoder, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(batch_size, epochs, drop_ratio, max_words=1000):\n",
    "    with mlflow.start_run():\n",
    "        # Log params\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"drop_ratio\", drop_ratio)\n",
    "        mlflow.log_param(\"max_words\", max_words)\n",
    "\n",
    "        # Prepare data\n",
    "        x_train, x_test, y_train, y_test, \\\n",
    "            tokenizer, encoder, num_classes = prepare_data(True, max_words)\n",
    "        \n",
    "        # Build model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dropout(drop_ratio))\n",
    "        model.add(layers.Dense(num_classes))\n",
    "        model.add(layers.Activation('softmax'))\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=0,\n",
    "            validation_split=0.1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        score = model.evaluate(\n",
    "            x_test, y_test,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"loss\", score[0])\n",
    "        mlflow.log_metric(\"accuracy\", score[1])\n",
    "        \n",
    "        # Save python model\n",
    "        mlflow.set_tag(\"python_model\", \"yes\")\n",
    "        mlflow.keras.log_model(model, \"model/python\")\n",
    "        \n",
    "        # Save javascript model\n",
    "        mlflow.set_tag(\"javascript_model\", \"yes\")\n",
    "        shutil.rmtree(\"/tmp/js-model\", ignore_errors=True)\n",
    "        tfjs.converters.save_keras_model(model, \"/tmp/js-model\")\n",
    "        mlflow.log_artifacts(\"/tmp/js-model\", \"model/javascript\")\n",
    "        \n",
    "        # Save bag of words\n",
    "        if os.path.exists(\"/tmp/bag_of_words.json\"):\n",
    "            os.remove(\"/tmp/bag_of_words.json\")\n",
    "        with open(\"/tmp/bag_of_words.json\", \"w\") as f:\n",
    "            json.dump(tokenizer.word_index, f)\n",
    "        mlflow.log_artifact(\"/tmp/bag_of_words.json\")\n",
    "        \n",
    "        # Save class labels\n",
    "        if os.path.exists(\"/tmp/label_classes.json\"):\n",
    "            os.remove(\"/tmp/label_classes.json\")\n",
    "        with open(\"/tmp/label_classes.json\", \"w\") as f:  \n",
    "            json.dump(list(encoder.classes_), f)\n",
    "        mlflow.log_artifact(\"/tmp/label_classes.json\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 4\n",
    "drop_ratio = 0.4\n",
    "max_words = 1000\n",
    "model = run_experiment(batch_size, epochs, drop_ratio, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = 1\n",
    "bucket_name = \"faculty-models\"\n",
    "def upload_to_gcloud(run_id):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    for path in [\n",
    "        \"bag_of_words.json\",\n",
    "        \"label_classes.json\",\n",
    "        \"model/javascript/model.json\",\n",
    "    ]:\n",
    "        name = os.path.basename(path)\n",
    "        local_path = f\"/tmp/{name}\"\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        datasets.get(\n",
    "            f\".mlflow-artifacts/{experiment_id}/{run_id}/{path}\",\n",
    "            local_path\n",
    "        )\n",
    "        blob = bucket.blob(name)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        blob.make_public()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcloud(\"352c0653-9dff-4e6b-acc1-15499e40190b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
